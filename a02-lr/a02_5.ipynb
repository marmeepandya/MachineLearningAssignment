{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1469b51e",
   "metadata": {},
   "source": [
    "# 5 Exploration (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01dcc2b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.utils.data\n",
    "import torch.nn.functional\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from a02_helper import *\n",
    "from a02_functions import optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67180ee9",
   "metadata": {},
   "source": [
    "### 5 Exploration: PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3b58a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to experiment, here is an implementation of logistic\n",
    "# regression in PyTorch\n",
    "\n",
    "# prepare the data\n",
    "Xztorch = torch.FloatTensor(Xz)\n",
    "ytorch = torch.LongTensor(y)\n",
    "train = torch.utils.data.TensorDataset(Xztorch, ytorch)\n",
    "\n",
    "\n",
    "# manual implementation of logistic regression (without bias)\n",
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, D, C):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.weights = torch.nn.Parameter(\n",
    "            torch.randn(D, C) / math.sqrt(D)\n",
    "        )  # xavier initialization\n",
    "        self.register_parameter(\"W\", self.weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.matmul(x, self.weights)\n",
    "        out = torch.nn.functional.log_softmax(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# define the objective and update function. here we ignore the learning rates and\n",
    "# parameters given to us by optimize (they are stored in the PyTorch model and\n",
    "# optimizer, resp., instead)\n",
    "def opt_pytorch():\n",
    "    model = LogisticRegression(D, 2)\n",
    "    criterion = torch.nn.NLLLoss(reduction=\"sum\")\n",
    "    # change the next line to try different optimizers\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    def objective(_):\n",
    "        outputs = model(Xztorch)\n",
    "        return criterion(outputs, ytorch)\n",
    "\n",
    "    def update(_1, _2):\n",
    "        for i, (examples, labels) in enumerate(train_loader):\n",
    "            outputs = model(examples)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        W = model.state_dict()[\"W\"]\n",
    "        w = W[:, 1] - W[:, 0]\n",
    "        return w\n",
    "\n",
    "    return (objective, update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb7621b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the optimizer\n",
    "learning_rate = 0.01\n",
    "batch_size = 100  # number of data points to sample for gradient estimate\n",
    "shuffle = True  # sample with replacement (false) or without replacement (true)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size, shuffle=True)\n",
    "wz_t, vz_t, _ = optimize(opt_pytorch(), None, nepochs=100, eps0=None, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fc8bb1",
   "metadata": {
    "lines_to_next_cell": 3
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ML25)",
   "language": "python",
   "name": "ml25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
